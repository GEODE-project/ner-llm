{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Token classification with OpenAI GPT models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "from utils import filter_ner_io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"GEODE/GeoEDdA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for key in dataset.keys():\n",
    "    dfs.append(pd.DataFrame({'dataset':key, 'text':dataset[key]['text'], 'meta':dataset[key]['meta'], 'tokens':dataset[key]['tokens'], 'spans':dataset[key]['spans']}))\n",
    "df = pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagset = ['Domain-mark','Head','NC-Person','NC-Spatial','NP-Misc','NP-Person','NP-Spatial','Relation','Latlong', 'ENE-Spatial', 'ENE-Person', 'ENE-Misc']\n",
    "\n",
    "df['ner_io'] = df.apply(lambda x: filter_ner_io(x, tagset), axis=1)\n",
    "\n",
    "df_train = df[df['dataset']=='train'].reset_index(drop = True)\n",
    "df_val = df[df['dataset']=='validation'].reset_index(drop = True)\n",
    "df_test = df[df['dataset']=='test'].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>text</th>\n",
       "      <th>meta</th>\n",
       "      <th>tokens</th>\n",
       "      <th>spans</th>\n",
       "      <th>ner_io</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train</td>\n",
       "      <td>ILLESCAS, (Géog.) petite ville d'Espagne, dans...</td>\n",
       "      <td>{'volume': 8, 'head': 'ILLESCAS', 'author': 'u...</td>\n",
       "      <td>[{'text': 'ILLESCAS', 'start': 0, 'end': 8, 'i...</td>\n",
       "      <td>[{'text': 'ILLESCAS', 'start': 0, 'end': 8, 't...</td>\n",
       "      <td>[[Head], [O], [O], [Domain-mark], [Domain-mark...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train</td>\n",
       "      <td>MULHAUSEN, (Géog.) ville impériale d'Allemagne...</td>\n",
       "      <td>{'volume': 10, 'head': 'MULHAUSEN', 'author': ...</td>\n",
       "      <td>[{'text': 'MULHAUSEN', 'start': 0, 'end': 9, '...</td>\n",
       "      <td>[{'text': 'MULHAUSEN', 'start': 0, 'end': 9, '...</td>\n",
       "      <td>[[Head], [O], [O], [Domain-mark], [Domain-mark...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train</td>\n",
       "      <td>* ADDA, riviere de Suisse &amp; d'Italie, qui a sa...</td>\n",
       "      <td>{'volume': 1, 'head': 'ADDA', 'author': 'Dider...</td>\n",
       "      <td>[{'text': '*', 'start': 0, 'end': 1, 'id': 0, ...</td>\n",
       "      <td>[{'text': 'ADDA', 'start': 2, 'end': 6, 'token...</td>\n",
       "      <td>[[O], [Head], [O], [ENE-Spatial, NC-Spatial], ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train</td>\n",
       "      <td>SINTRA ou CINTRA, (Géog. mod.) montagne de Por...</td>\n",
       "      <td>{'volume': 15, 'head': 'SINTRA ou CINTRA', 'au...</td>\n",
       "      <td>[{'text': 'SINTRA', 'start': 0, 'end': 6, 'id'...</td>\n",
       "      <td>[{'text': 'SINTRA ou CINTRA', 'start': 0, 'end...</td>\n",
       "      <td>[[Head], [Head], [Head], [O], [O], [Domain-mar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train</td>\n",
       "      <td>* ACHSTEDE, ou AKSTEDE, s. petite Ville d'Alle...</td>\n",
       "      <td>{'volume': 1, 'head': 'ACHSTEDE, ou AKSTEDE', ...</td>\n",
       "      <td>[{'text': '*', 'start': 0, 'end': 1, 'id': 0, ...</td>\n",
       "      <td>[{'text': 'ACHSTEDE, ou AKSTEDE', 'start': 2, ...</td>\n",
       "      <td>[[O], [Head], [Head], [Head], [Head], [O], [O]...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  dataset                                               text  \\\n",
       "0   train  ILLESCAS, (Géog.) petite ville d'Espagne, dans...   \n",
       "1   train  MULHAUSEN, (Géog.) ville impériale d'Allemagne...   \n",
       "2   train  * ADDA, riviere de Suisse & d'Italie, qui a sa...   \n",
       "3   train  SINTRA ou CINTRA, (Géog. mod.) montagne de Por...   \n",
       "4   train  * ACHSTEDE, ou AKSTEDE, s. petite Ville d'Alle...   \n",
       "\n",
       "                                                meta  \\\n",
       "0  {'volume': 8, 'head': 'ILLESCAS', 'author': 'u...   \n",
       "1  {'volume': 10, 'head': 'MULHAUSEN', 'author': ...   \n",
       "2  {'volume': 1, 'head': 'ADDA', 'author': 'Dider...   \n",
       "3  {'volume': 15, 'head': 'SINTRA ou CINTRA', 'au...   \n",
       "4  {'volume': 1, 'head': 'ACHSTEDE, ou AKSTEDE', ...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [{'text': 'ILLESCAS', 'start': 0, 'end': 8, 'i...   \n",
       "1  [{'text': 'MULHAUSEN', 'start': 0, 'end': 9, '...   \n",
       "2  [{'text': '*', 'start': 0, 'end': 1, 'id': 0, ...   \n",
       "3  [{'text': 'SINTRA', 'start': 0, 'end': 6, 'id'...   \n",
       "4  [{'text': '*', 'start': 0, 'end': 1, 'id': 0, ...   \n",
       "\n",
       "                                               spans  \\\n",
       "0  [{'text': 'ILLESCAS', 'start': 0, 'end': 8, 't...   \n",
       "1  [{'text': 'MULHAUSEN', 'start': 0, 'end': 9, '...   \n",
       "2  [{'text': 'ADDA', 'start': 2, 'end': 6, 'token...   \n",
       "3  [{'text': 'SINTRA ou CINTRA', 'start': 0, 'end...   \n",
       "4  [{'text': 'ACHSTEDE, ou AKSTEDE', 'start': 2, ...   \n",
       "\n",
       "                                              ner_io  \n",
       "0  [[Head], [O], [O], [Domain-mark], [Domain-mark...  \n",
       "1  [[Head], [O], [O], [Domain-mark], [Domain-mark...  \n",
       "2  [[O], [Head], [O], [ENE-Spatial, NC-Spatial], ...  \n",
       "3  [[Head], [Head], [Head], [O], [O], [Domain-mar...  \n",
       "4  [[O], [Head], [Head], [Head], [Head], [O], [O]...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_set_train = []\n",
    "for index,row in df_train.iterrows():\n",
    "    flat_curr=[element for elements in row['ner_io'] for element in elements]\n",
    "    examples_set_train.append(len(set(flat_curr)))\n",
    "\n",
    "idx = np.flip(np.argsort(examples_set_train))[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data structure\n",
    "class Entity(BaseModel):\n",
    "    text: str = Field(description=\"Texte du token appartenant à une entité (ex : 'ville', 'France')\")\n",
    "    #label return a list of labels\n",
    "    labels: List[str] = Field(description=\"Liste des labels de l'entité (ex : ['Domain-mark'], ['NP-Spatial', 'ENE-Spatial'])\")\n",
    "    #label: str = Field(description=\"Label de l'entité, exclusivement parmi la liste suivante : ['Domain-mark', 'Head', 'NC-Person', 'NC-Spatial', 'NP-Misc', 'NP-Person', 'NP-Spatial', 'Relation','Latlong', 'ENE-Spatial', 'ENE-Person', 'ENE-Misc','O'] \")\n",
    "\n",
    "class Entities(BaseModel):\n",
    "    entities: List[Entity] = Field(description=\"The token contained in the provided context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples=f'''Voici quelques exemples d'entrées et de sorties attendues pour la tâche d'identification des entités dans un texte :\n",
    "EXEMPLE 0:\n",
    "    INPUT:{' '.join([\"('\"+token['text']+\"' ,\"+str(id)+\")\" for id,token in enumerate(df_train.iloc[idx[1]]['tokens'])])}\n",
    "    OUTPUT:{[{'label':tag,'text':token['text']} for tag,token in zip(df_train.iloc[idx[1]]['ner_io'],df_train.iloc[idx[1]]['tokens'])]}\n",
    "---\n",
    "'''\n",
    "\n",
    "directives=f'''\n",
    "Tu es un expert en Traitement Automatique du Langage Naturel. Ta tâche est d'identifier les entités (entités nommées, entités nominales et entités imbriquées) dans un texte donné. Les textes sont des paragraphes issus d'articles de l'enyclopédie de Diderot et d'Alembert, et les entités sont des tokens qui peuvent être des noms propres, des noms communs, des relations spatiales ou des coordonnées géographiques.\n",
    "Les types d'entités possibles sont exclusivement : (Domain-mark, Head, NC-Person, NC-Spatial, NP-Misc, NP-Person, NP-Spatial, Relation, Latlong, ENE-Spatial, ENE-Person, ENE-Misc, O) et peuvent être décrits comme suit :\n",
    "1. Domain-mark : token indiquant le domaine de connaissance (généralement après le head et entre parenthèses). Par exemple : 'Géog., Géog. mod., Géog. anc., Géogr., Géogr. mod., Marine., Hist. nat., Gram., Géogr. anc., Jurisprud., Géog. anc. & mod., Gramm., Geog.'\n",
    "2. Head : nom de l'entrée ou article encyclopédique au début de la phrase et est presque toujours en majuscules tel que 'Aire, Afrique, Aigle, ILLESCAS, MULHAUSEN, ADDA, SINTRA ou CINTRA, ACHSTEDE, ou AKSTEDE, KEITH, CAÇERES, CARMAGNOLE, AGRIGNON, INSPRUCK'\n",
    "3. NC-Person : un nom commun qui identifie une personne telle que 'M., roi, S., peuples, l'empereur, son fils, les habitans, prince, peuple, le roi, fils, le P., habitans'\n",
    "4. NC-Spatial : un nom commun qui identifie une entité spatiale y compris les caractéristiques naturelles telles que 'ville, petite ville, la riviere, la mer, royaume, la province, capitale, la ville, l'île, cette ville, pays, la côte, riviere'\n",
    "5. NP-Misc : un nom propre identifiant des entités non classées comme spatiales ou personnelles telles que 'l'Eglise, grec, 1707, russien, Glaciale, Noire, romain, la Croix, Russien, Parlement, 1693, Sud, 1614'\n",
    "6. NP-Person : un nom propre identifiant le nom d'une personne (entités nommées de personnes) telles que 'Ptolomée, Pline, Strabon, Euripide, les Romains, Pierre, Romains, les Anglois, Turcs, Dieu, César, Antonin, les Espagnols'\n",
    "7. NP-Spatial : un nom propre identifiant le nom d'un lieu (entités nommées spatiales) telles que 'France, Allemagne, Italie, Espagne, Afrique, Asie, Paris, Naples, Angleterre, Rome, Russie, la Chine, l'Amérique méridionale'\n",
    "8. Relation : relation spatiale telle que 'dans, sur, au, en, entre, près de, se jette dans, proche, par, vers, près du, jusqu'à, à l'orient'.\n",
    "9. Latlong : coordonnées géographiques telles que 'Long. 31. 58. lat. 40. 55', 'Long. 10. 27. lat. 43. 30', 'Long. selon Harris, 29. 16. 15. lat. 47. 15'.\n",
    "10. ENE-Spatial : entité nommée spatiale imbriquée, qui est une entité spatiale qui est composé par d'autres entités, par exemple 'la ville de Paris', 'la province de Bretagne', 'ville de France'.\n",
    "11. ENE-Person : entité nommée personne imbriquée, qui est une entité faisant référence à une personne et est composé par d'autres entités, par exemple 'le czar Pierre', 'roi de Macédoine'.\n",
    "12. ENE-Misc : entité nommée non spatiale ou personne imbriquée, qui est une entité faisant référence à un nom propre et est composé par d'autres entités, par exemple \"l'ordre de S. Jacques\", 'la déclaration du 21 Mars 1671'.\n",
    "10. O : ce token n'appartient à aucune entité.\n",
    "\n",
    "{examples}\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_gpt(model, input, directives):\n",
    "    context = f'''{' '.join([\"('\" + token['text'] + \"' ,\" + str(id) + \")\" for id, token in enumerate(input)])}'''\n",
    "    #print(context)\n",
    "    template = f'''context:{context}\n",
    "    query:{{query}}\n",
    "    format_instructions:{{format_instructions}}\n",
    "    '''\n",
    "    # Set up a parser + inject instructions into the prompt template.\n",
    "    parser = JsonOutputParser(pydantic_object=Entities)\n",
    "    prompt = PromptTemplate(\n",
    "        template=template,\n",
    "        input_variables=[\"query\"],\n",
    "        partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "    )\n",
    "    chain = prompt | model | parser\n",
    "    return chain.invoke({\"query\": directives})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 'gpt4o' # 'gpt3.5', 'gpt4', 'gpt4o', 'o1-mini', 'gpt4.1-mini'\n",
    "model_name = 'gpt-4o-mini-2024-07-18' #'gpt-3.5-turbo', 'gpt-4', 'gpt-4o-mini-2024-07-18', 'gpt-4o', 'o1-preview', 'o1-mini-2024-09-12', 'gpt-4.1-mini-2025-04-14'\n",
    "\n",
    "model = ChatOpenAI(temperature=1, model=model_name)\n",
    "nb_iterations = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model with the first entry of the test set and check the output\n",
    "#run_gpt(model, df_test.iloc[0]['tokens'], directives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = os.path.join('predictions', 'token_classification_' + version)\n",
    "\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [1:05:35<00:00, 19.68s/it]\n"
     ]
    }
   ],
   "source": [
    "index = range(0, len(df_test.index))\n",
    "for i in range(nb_iterations):\n",
    "    path = os.path.join(output_path, \"run_\" + str(i+1))\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    for j in tqdm(index):\n",
    "        # if entry_j alreay exists, skip it\n",
    "        if os.path.exists(os.path.join(path, f\"entry_{j:03d}.json\")):\n",
    "            #print(f\"Skipping index {j}, already exists.\")\n",
    "            continue\n",
    "        try:\n",
    "            output = run_gpt(model, df_test.iloc[j]['tokens'], directives)\n",
    "        except Exception as e:\n",
    "            print(f\"Error for index {j}: {e}\")\n",
    "            print(f\"Retry\")\n",
    "            try:\n",
    "                output = run_gpt(model, df_test.iloc[j]['tokens'], directives)\n",
    "            except Exception as e:\n",
    "                print(f\"Error for index {j}: {e}\")\n",
    "                output = {'entities':[]}\n",
    "    \n",
    "        # Save the output to a file, the name of the file is entry_j.json with j on 3 digits\n",
    "        with open(os.path.join(path, f\"entry_{j:03d}.json\"), 'w') as file:\n",
    "            json.dump(output, file)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ner-llm-py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
