{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Token Classification with OpenAI GPT Models\n",
    "\n",
    "\n",
    "## Import Python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dataset\n",
    "\n",
    "In this experiment, we use the GeoEDdA dataset which contains semantic annotations (at the token and span levels) for named entities (i.e., Spatial, Person, and Misc), nominal entities, spatial relations, and geographic coordinates. Nested named entities also present in this dataset were not considered in this experiment.\n",
    "\n",
    "The dataset is available in the HuggingFace Hub: https://huggingface.co/datasets/GEODE/GeoEDdA\n",
    "\n",
    "* Load the GeoEDdA dataset from the HuggingFace Hub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"GEODE/GeoEDdA\")\n",
    "test_set = pd.DataFrame(dataset['test'])\n",
    "test_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagset = ['Domain-mark','Head','NC-Person','NC-Spatial','NP-Misc','NP-Person','NP-Spatial','Relation','Latlong']\n",
    "\n",
    "def filter_annotations(doc, tagset):\n",
    "    result = ['O'] * len(doc['tokens'])\n",
    "    for span in doc['spans']:\n",
    "        if(span['label'] in tagset):\n",
    "            for i in range(span['token_start'], span['token_end'] + 1):\n",
    "                if(result[i] == 'O' or span['label'] == 'Latlong'):\n",
    "                    result[i] = span['label']\n",
    "                elif(result[i] == ['Latlong']):\n",
    "                    break\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Add a new column with the list of tags (one tag per token):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set['tags'] = test_set.apply(lambda x: filter_annotations(x, tagset), axis=1)\n",
    "test_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several iterations of the token classification task have been performed. The predictions from all the iterations can be loaded and evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_predictions(path):\n",
    "    files = sorted([f for f in listdir(path) if isfile(join(path, f))])\n",
    "    predictions = [] # contains the predictions of each iteration\n",
    "    for file in files:\n",
    "        with open(join(path, file), encoding='utf-8') as f:\n",
    "            predictions.append(json.load(f))\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def formatting_ner(pred_sentence, true_sentence):\n",
    "    formatted_pred_sentence = []\n",
    "    i = 0\n",
    "    j = 0\n",
    "    while(i < len(pred_sentence['entities']) and j < len(true_sentence['tokens'])):\n",
    "        if('text' not in list(pred_sentence['entities'][i].keys())):\n",
    "            formatted_pred_sentence.append('O')\n",
    "            i += 1\n",
    "            j += 1\n",
    "        elif(pred_sentence['entities'][i]['text'] == true_sentence['tokens'][j]['text'] and 'label' in list(pred_sentence['entities'][i].keys())):\n",
    "            formatted_pred_sentence.append(pred_sentence['entities'][i]['label'])\n",
    "            i += 1\n",
    "            j += 1\n",
    "        else:\n",
    "            j += 1\n",
    "            formatted_pred_sentence.append('O')\n",
    "    while(j < len(true_sentence['tokens'])):\n",
    "        formatted_pred_sentence.append('O')\n",
    "        j += 1\n",
    "    return formatted_pred_sentence\n",
    "\n",
    "\n",
    "def format_sentences(pred, true):\n",
    "    formatted_pred_sentences = []\n",
    "    for pred_sentence, true_sentence in zip(pred, true.iterrows()):\n",
    "        formatted_pred_sentences.append(formatting_ner(pred_sentence,true_sentence[1]))\n",
    "    return formatted_pred_sentences\n",
    "\n",
    "\n",
    "def evaluate(predictions, test_set):\n",
    "    classification_reports = []\n",
    "    for iteration_predictions in predictions:\n",
    "        formatted_sentences = format_sentences(iteration_predictions, test_set)\n",
    "        pred_sentences_flat = [element for pred_sentences in formatted_sentences for element in pred_sentences]\n",
    "        trues_flat = [element for true_sentence in list(test_set['tags']) for element in true_sentence]\n",
    "        classification_reports.append(classification_report(trues_flat, pred_sentences_flat, output_dict=True, digits=3, labels=[tag for tag in tagset if tag != 'O']))\n",
    "    return classification_reports\n",
    "\n",
    "\n",
    "def get_avg_scores(classification_reports, score='f1-score'):\n",
    "    avg_scores = {}\n",
    "    for report in classification_reports:\n",
    "        for tag in report.keys():\n",
    "            if tag not in avg_scores:\n",
    "                avg_scores[tag] = []\n",
    "            avg_scores[tag].append(report[tag][score])\n",
    "\n",
    "    avg_scores = {tag: sum(scores)/len(scores) for tag, scores in avg_scores.items()}\n",
    "    return avg_scores\n",
    "\n",
    "\n",
    "def bar_plot(scores, tagset, score='f1-score'):\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    barWidth = 0.25\n",
    "    bars = [[data[tag] for tag in tagset if tag != 'O'] for data in scores.values()]\n",
    "    r = [range(len(bars[0]))]\n",
    "    for i in range(1, len(bars)):\n",
    "        r.append([x + barWidth + 0.02 for x in r[i-1]])\n",
    "    colors = ['#0072B2', '#D55E00', '#CC79A7', '#E69F00', '#56B4E9'] # Colorblind-friendly palette\n",
    "    for i in range(len(bars)):\n",
    "        ax.bar(r[i], bars[i], color=colors[i], width=barWidth, label=f'{list(scores.keys())[i]}')\n",
    "    ax.set_xticks([r + barWidth for r in range(len(bars[0]))])\n",
    "    ax.set_xticklabels(tagset, rotation=30)\n",
    "    ax.tick_params(bottom=False, left=False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.spines['bottom'].set_color('#DDDDDD')\n",
    "    ax.set_axisbelow(True)\n",
    "    ax.yaxis.grid(True, color='#EEEEEE')\n",
    "    ax.xaxis.grid(False)\n",
    "    ax.set_xlabel('Token classes', labelpad=15, color='#333333')\n",
    "    ax.set_ylabel(score, labelpad=15, color='#333333')\n",
    "    plt.legend(loc=(1.04, 0.7))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['gpt3.5', 'gpt4', 'gpt4o']\n",
    "metrics = ['precision', 'recall', 'f1-score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Get classification report for GPT 3.5: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = join('predictions','token_classification_' + models[0])\n",
    "\n",
    "predictions = load_predictions(path)\n",
    "classification_reports = evaluate(predictions, test_set)\n",
    "\n",
    "# display report of the first iteration\n",
    "classification_reports[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Display average f1-scores for each tag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_avg_scores(classification_reports, score=metrics[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Get micro average scores for each model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    path = join('predictions','token_classification_' + model)\n",
    "    predictions = load_predictions(path)\n",
    "    classification_reports = evaluate(predictions, test_set)\n",
    "    \n",
    "    print(f'{model}', end=':\\t')\n",
    "\n",
    "    for metric in metrics:\n",
    "        scores = get_avg_scores(classification_reports, metric)\n",
    "        print(f'{metric}', end=': ')\n",
    "        print(scores['micro avg'], end='\\t')\n",
    "        #print(\"{:.2f}\".format(scores['micro avg']), end='\\t')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Plot average scores for each GPT model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_scores = {}\n",
    "\n",
    "for metric in metrics:\n",
    "    for model in models:\n",
    "        path = join('predictions','token_classification_' + model)\n",
    "        predictions = load_predictions(path)\n",
    "        classification_reports = evaluate(predictions, test_set)\n",
    "        eval_scores[model] = get_avg_scores(classification_reports, metric)\n",
    "\n",
    "    bar_plot(eval_scores, tagset, metric)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Comparison with a [fine-tuned BERT](https://huggingface.co/GEODE/bert-base-french-cased-edda-ner) scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_f1scores =  {\n",
    "    'Domain-mark': 0.99,\n",
    "    'Head': 0.98,\n",
    "    'NC-Person': 0.75,\n",
    "    'NC-Spatial': 0.93,\n",
    "    'NP-Misc': 0.76,\n",
    "    'NP-Person': 0.88,\n",
    "    'NP-Spatial': 0.95,\n",
    "    'Relation': 0.91,\n",
    "    'Latlong': 0.98\n",
    "}\n",
    "\n",
    "eval_scores = {}\n",
    "\n",
    "for model in models:\n",
    "    path = join('predictions','token_classification_' + model)\n",
    "    predictions = load_predictions(path)\n",
    "    classification_reports = evaluate(predictions, test_set)\n",
    "    eval_scores[model] = get_avg_scores(classification_reports, metric)\n",
    "\n",
    "eval_scores['fine-tuned BERT'] = bert_f1scores\n",
    "\n",
    "bar_plot(eval_scores, tagset, metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt-py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
