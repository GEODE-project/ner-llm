{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Token Classification with OpenAI GPT Models\n",
    "\n",
    "\n",
    "## Import Python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datasets import load_dataset\n",
    "from utils import filter_ner_io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dataset\n",
    "\n",
    "In this experiment, we use the GeoEDdA dataset which contains semantic annotations (at the token and span levels) for named entities (i.e., Spatial, Person, and Misc), nominal entities, spatial relations, and geographic coordinates. Nested named entities also present in this dataset were not considered in this experiment.\n",
    "\n",
    "The dataset is available in the HuggingFace Hub: https://huggingface.co/datasets/GEODE/GeoEDdA\n",
    "\n",
    "* Load the GeoEDdA dataset from the HuggingFace Hub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>meta</th>\n",
       "      <th>tokens</th>\n",
       "      <th>spans</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>COMPIEGNE, (Géog. mod.) ville de France, dans ...</td>\n",
       "      <td>{'volume': 3, 'head': 'COMPIEGNE', 'author': '...</td>\n",
       "      <td>[{'text': 'COMPIEGNE', 'start': 0, 'end': 9, '...</td>\n",
       "      <td>[{'text': 'COMPIEGNE', 'start': 0, 'end': 9, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HILPERHAUSEN, (Géog.) ville d'Allemagne en Fra...</td>\n",
       "      <td>{'volume': 8, 'head': 'HILPERHAUSEN', 'author'...</td>\n",
       "      <td>[{'text': 'HILPERHAUSEN', 'start': 0, 'end': 1...</td>\n",
       "      <td>[{'text': 'HILPERHAUSEN', 'start': 0, 'end': 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Patane ou Patany, (Géog. mod.) royaume des Ind...</td>\n",
       "      <td>{'volume': 12, 'head': 'Patane ou Patany', 'au...</td>\n",
       "      <td>[{'text': 'Patane', 'start': 0, 'end': 6, 'id'...</td>\n",
       "      <td>[{'text': 'Patane ou Patany', 'start': 0, 'end...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>* ABYDE ou ABYDOS, sub. Ville maritime de Phry...</td>\n",
       "      <td>{'volume': 1, 'head': 'ABYDE ou ABYDOS', 'auth...</td>\n",
       "      <td>[{'text': '*', 'start': 0, 'end': 1, 'id': 0, ...</td>\n",
       "      <td>[{'text': 'ABYDE ou ABYDOS', 'start': 2, 'end'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DUSSELDORP, (Géog. mod.) ville du cercle de We...</td>\n",
       "      <td>{'volume': 5, 'head': 'DUSSELDORP', 'author': ...</td>\n",
       "      <td>[{'text': 'DUSSELDORP', 'start': 0, 'end': 10,...</td>\n",
       "      <td>[{'text': 'DUSSELDORP', 'start': 0, 'end': 10,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  COMPIEGNE, (Géog. mod.) ville de France, dans ...   \n",
       "1  HILPERHAUSEN, (Géog.) ville d'Allemagne en Fra...   \n",
       "2  Patane ou Patany, (Géog. mod.) royaume des Ind...   \n",
       "3  * ABYDE ou ABYDOS, sub. Ville maritime de Phry...   \n",
       "4  DUSSELDORP, (Géog. mod.) ville du cercle de We...   \n",
       "\n",
       "                                                meta  \\\n",
       "0  {'volume': 3, 'head': 'COMPIEGNE', 'author': '...   \n",
       "1  {'volume': 8, 'head': 'HILPERHAUSEN', 'author'...   \n",
       "2  {'volume': 12, 'head': 'Patane ou Patany', 'au...   \n",
       "3  {'volume': 1, 'head': 'ABYDE ou ABYDOS', 'auth...   \n",
       "4  {'volume': 5, 'head': 'DUSSELDORP', 'author': ...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [{'text': 'COMPIEGNE', 'start': 0, 'end': 9, '...   \n",
       "1  [{'text': 'HILPERHAUSEN', 'start': 0, 'end': 1...   \n",
       "2  [{'text': 'Patane', 'start': 0, 'end': 6, 'id'...   \n",
       "3  [{'text': '*', 'start': 0, 'end': 1, 'id': 0, ...   \n",
       "4  [{'text': 'DUSSELDORP', 'start': 0, 'end': 10,...   \n",
       "\n",
       "                                               spans  \n",
       "0  [{'text': 'COMPIEGNE', 'start': 0, 'end': 9, '...  \n",
       "1  [{'text': 'HILPERHAUSEN', 'start': 0, 'end': 1...  \n",
       "2  [{'text': 'Patane ou Patany', 'start': 0, 'end...  \n",
       "3  [{'text': 'ABYDE ou ABYDOS', 'start': 2, 'end'...  \n",
       "4  [{'text': 'DUSSELDORP', 'start': 0, 'end': 10,...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"GEODE/GeoEDdA\")\n",
    "test_set = pd.DataFrame(dataset['test'])\n",
    "test_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Add a new column with the list of tags (one tag per token):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>meta</th>\n",
       "      <th>tokens</th>\n",
       "      <th>spans</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>COMPIEGNE, (Géog. mod.) ville de France, dans ...</td>\n",
       "      <td>{'volume': 3, 'head': 'COMPIEGNE', 'author': '...</td>\n",
       "      <td>[{'text': 'COMPIEGNE', 'start': 0, 'end': 9, '...</td>\n",
       "      <td>[{'text': 'COMPIEGNE', 'start': 0, 'end': 9, '...</td>\n",
       "      <td>[[Head], [O], [O], [Domain-mark], [Domain-mark...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HILPERHAUSEN, (Géog.) ville d'Allemagne en Fra...</td>\n",
       "      <td>{'volume': 8, 'head': 'HILPERHAUSEN', 'author'...</td>\n",
       "      <td>[{'text': 'HILPERHAUSEN', 'start': 0, 'end': 1...</td>\n",
       "      <td>[{'text': 'HILPERHAUSEN', 'start': 0, 'end': 1...</td>\n",
       "      <td>[[Head], [O], [O], [Domain-mark], [Domain-mark...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Patane ou Patany, (Géog. mod.) royaume des Ind...</td>\n",
       "      <td>{'volume': 12, 'head': 'Patane ou Patany', 'au...</td>\n",
       "      <td>[{'text': 'Patane', 'start': 0, 'end': 6, 'id'...</td>\n",
       "      <td>[{'text': 'Patane ou Patany', 'start': 0, 'end...</td>\n",
       "      <td>[[Head], [Head], [Head], [O], [O], [Domain-mar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>* ABYDE ou ABYDOS, sub. Ville maritime de Phry...</td>\n",
       "      <td>{'volume': 1, 'head': 'ABYDE ou ABYDOS', 'auth...</td>\n",
       "      <td>[{'text': '*', 'start': 0, 'end': 1, 'id': 0, ...</td>\n",
       "      <td>[{'text': 'ABYDE ou ABYDOS', 'start': 2, 'end'...</td>\n",
       "      <td>[[O], [Head], [Head], [Head], [O], [O], [O], [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DUSSELDORP, (Géog. mod.) ville du cercle de We...</td>\n",
       "      <td>{'volume': 5, 'head': 'DUSSELDORP', 'author': ...</td>\n",
       "      <td>[{'text': 'DUSSELDORP', 'start': 0, 'end': 10,...</td>\n",
       "      <td>[{'text': 'DUSSELDORP', 'start': 0, 'end': 10,...</td>\n",
       "      <td>[[Head], [O], [O], [Domain-mark], [Domain-mark...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  COMPIEGNE, (Géog. mod.) ville de France, dans ...   \n",
       "1  HILPERHAUSEN, (Géog.) ville d'Allemagne en Fra...   \n",
       "2  Patane ou Patany, (Géog. mod.) royaume des Ind...   \n",
       "3  * ABYDE ou ABYDOS, sub. Ville maritime de Phry...   \n",
       "4  DUSSELDORP, (Géog. mod.) ville du cercle de We...   \n",
       "\n",
       "                                                meta  \\\n",
       "0  {'volume': 3, 'head': 'COMPIEGNE', 'author': '...   \n",
       "1  {'volume': 8, 'head': 'HILPERHAUSEN', 'author'...   \n",
       "2  {'volume': 12, 'head': 'Patane ou Patany', 'au...   \n",
       "3  {'volume': 1, 'head': 'ABYDE ou ABYDOS', 'auth...   \n",
       "4  {'volume': 5, 'head': 'DUSSELDORP', 'author': ...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [{'text': 'COMPIEGNE', 'start': 0, 'end': 9, '...   \n",
       "1  [{'text': 'HILPERHAUSEN', 'start': 0, 'end': 1...   \n",
       "2  [{'text': 'Patane', 'start': 0, 'end': 6, 'id'...   \n",
       "3  [{'text': '*', 'start': 0, 'end': 1, 'id': 0, ...   \n",
       "4  [{'text': 'DUSSELDORP', 'start': 0, 'end': 10,...   \n",
       "\n",
       "                                               spans  \\\n",
       "0  [{'text': 'COMPIEGNE', 'start': 0, 'end': 9, '...   \n",
       "1  [{'text': 'HILPERHAUSEN', 'start': 0, 'end': 1...   \n",
       "2  [{'text': 'Patane ou Patany', 'start': 0, 'end...   \n",
       "3  [{'text': 'ABYDE ou ABYDOS', 'start': 2, 'end'...   \n",
       "4  [{'text': 'DUSSELDORP', 'start': 0, 'end': 10,...   \n",
       "\n",
       "                                                tags  \n",
       "0  [[Head], [O], [O], [Domain-mark], [Domain-mark...  \n",
       "1  [[Head], [O], [O], [Domain-mark], [Domain-mark...  \n",
       "2  [[Head], [Head], [Head], [O], [O], [Domain-mar...  \n",
       "3  [[O], [Head], [Head], [Head], [O], [O], [O], [...  \n",
       "4  [[Head], [O], [O], [Domain-mark], [Domain-mark...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagset = ['Domain-mark','Head','NC-Person','NC-Spatial','NP-Misc','NP-Person','NP-Spatial','Relation','Latlong', 'ENE-Spatial', 'ENE-Person', 'ENE-Misc']\n",
    "\n",
    "test_set['tags'] = test_set.apply(lambda x: filter_ner_io(x, tagset), axis=1)\n",
    "test_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several iterations of the token classification task have been performed. The predictions from all the iterations can be loaded and evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import numpy as np\n",
    "\n",
    "def load_predictions(path):\n",
    "    predictions = []\n",
    "    for run in sorted(listdir(path)):\n",
    "        files = sorted([f for f in listdir(join(path, run)) if isfile(join(path, run, f))])\n",
    "        preds = []\n",
    "        #print(files)\n",
    "        for file in files:\n",
    "            with open(join(path,run, file), encoding='utf-8') as f:\n",
    "                preds.append(json.load(f))\n",
    "        predictions.append(preds)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "def formatting_ner(pred_sentence, true_sentence):\n",
    "    formatted_pred_sentence = []\n",
    "    i = 0\n",
    "    j = 0\n",
    "\n",
    "    if pred_sentence is not None:\n",
    "        pred_entities = pred_sentence.get('entities', [])\n",
    "        while i < len(pred_entities) and j < len(true_sentence['tokens']):\n",
    "            pred_token = pred_entities[i]\n",
    "            true_token = true_sentence['tokens'][j]\n",
    "\n",
    "            if 'text' not in pred_token or pred_token['text'] != true_token['text']:\n",
    "                formatted_pred_sentence.append(['O'])\n",
    "                j += 1\n",
    "                continue\n",
    "\n",
    "            labels = pred_token.get('labels', pred_token.get('label', ['O']))\n",
    "            if isinstance(labels, str):\n",
    "                labels = [labels]\n",
    "            formatted_pred_sentence.append(sorted(labels))\n",
    "            i += 1\n",
    "            j += 1\n",
    "\n",
    "    # Handle remaining true tokens\n",
    "    while j < len(true_sentence['tokens']):\n",
    "        formatted_pred_sentence.append(['O'])\n",
    "        j += 1\n",
    "\n",
    "    return formatted_pred_sentence\n",
    "\n",
    "#def formatting_ner(pred_sentence, true_sentence):\n",
    "#    formatted_pred_sentence = []\n",
    "#    i = 0\n",
    "#    j = 0\n",
    "#    if pred_sentence is not None:\n",
    "#        while(i < len(pred_sentence['entities']) and j < len(true_sentence['tokens'])):\n",
    "#            if('text' not in list(pred_sentence['entities'][i].keys())):\n",
    "#                formatted_pred_sentence.append('O')\n",
    "#                i += 1\n",
    "#                j += 1\n",
    "#            elif(pred_sentence['entities'][i]['text'] == true_sentence['tokens'][j]['text'] and 'labels' in list(pred_sentence['entities'][i].keys())):\n",
    "#                formatted_pred_sentence.append(pred_sentence['entities'][i]['labels'])\n",
    "#                i += 1\n",
    "#                j += 1\n",
    "#            else:\n",
    "#                j += 1\n",
    "#                formatted_pred_sentence.append('O')\n",
    "#    while(j < len(true_sentence['tokens'])):\n",
    "#        formatted_pred_sentence.append('O')\n",
    "#        j += 1\n",
    "#    return formatted_pred_sentence\n",
    "\n",
    "\n",
    "def format_sentences(pred, true):\n",
    "    formatted_pred_sentences = []\n",
    "    for pred_sentence, true_sentence in zip(pred, true.iterrows()):\n",
    "        formatted_pred_sentences.append(formatting_ner(pred_sentence,true_sentence[1]))\n",
    "    return formatted_pred_sentences\n",
    "\n",
    "\n",
    "def flatten_multilabel(true_tags_column):\n",
    "    \"\"\"\n",
    "    Flattens a list of list of tags (sentence → tokens → tag(s)) into\n",
    "    a flat list of sets per token, handling both list-of-labels and single-label formats.\n",
    "    \n",
    "    Case 1: tag is a list of labels → {'NP-Spatial', 'ENE-Spatial'}\n",
    "    Case 2: tag is a single label → {'NP-Spatial'}\n",
    "    \"\"\"\n",
    "    all_tags = []\n",
    "    for sentence in true_tags_column:\n",
    "        for token_tag in sentence:\n",
    "            if isinstance(token_tag, list):\n",
    "                # Case 1: Already a list of labels\n",
    "                all_tags.append(set(token_tag))\n",
    "            elif isinstance(token_tag, str):\n",
    "                # Case 2: Single label\n",
    "                all_tags.append(set([token_tag]))\n",
    "            else:\n",
    "                # Fallback\n",
    "                all_tags.append(set())\n",
    "    return all_tags\n",
    "\n",
    "def format_predictions_multilabel(predictions, test_set):\n",
    "    \"\"\"\n",
    "    Wraps predicted single-labels in sets, treating 'O' as empty set.\n",
    "    \"\"\"\n",
    "    formatted_pred_sentences = format_sentences(predictions, test_set)\n",
    "    return [\n",
    "        set([tag]) if isinstance(tag, str) and tag != 'O' else set()\n",
    "        for sentence in formatted_pred_sentences for tag in sentence\n",
    "    ]\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_multilabel(predictions, test_set, tagset):\n",
    "    \"\"\"\n",
    "    Évalue un modèle NER multilabel en comparant les prédictions avec les vraies étiquettes.\n",
    "    Prend en charge les cas où les étiquettes sont soit des chaînes uniques soit des listes de labels.\n",
    "    \"\"\"\n",
    "\n",
    "    all_true_labels = []\n",
    "    all_pred_labels = []\n",
    "\n",
    "    for pred_sentence, (_, true_sentence) in zip(predictions, test_set.iterrows()):\n",
    "        pred_formatted = formatting_ner(pred_sentence, true_sentence)\n",
    "        true_formatted = true_sentence['tags']\n",
    "\n",
    "        # Assurer cohérence : transformer chaque tag en set\n",
    "        pred_sets = [set(tags) if isinstance(tags, list) else set([tags]) for tags in pred_formatted]\n",
    "        true_sets = [set(tags) if isinstance(tags, list) else set([tags]) for tags in true_formatted]\n",
    "\n",
    "        all_pred_labels.extend(pred_sets)\n",
    "        all_true_labels.extend(true_sets)\n",
    "\n",
    "    # Binarisation multilabel\n",
    "    mlb = MultiLabelBinarizer(classes=sorted(set(tagset)))\n",
    "    y_true = mlb.fit_transform(all_true_labels)\n",
    "    y_pred = mlb.transform(all_pred_labels)\n",
    "\n",
    "    # Calcul des scores\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(y_true, y_pred, average=None, zero_division=0)\n",
    "    macro_precision, macro_recall, macro_f1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro', zero_division=0)\n",
    "    micro_precision, micro_recall, micro_f1, _ = precision_recall_fscore_support(y_true, y_pred, average='micro', zero_division=0)\n",
    "\n",
    "    # Organisation des scores\n",
    "    metrics = {\n",
    "        tag: {\n",
    "            'precision': p,\n",
    "            'recall': r,\n",
    "            'f1-score': f,\n",
    "            'support': s\n",
    "        } for tag, p, r, f, s in zip(mlb.classes_, precision, recall, f1, support)\n",
    "    }\n",
    "\n",
    "    # Ajout des moyennes globales\n",
    "    metrics['macro avg'] = {\n",
    "        'precision': macro_precision,\n",
    "        'recall': macro_recall,\n",
    "        'f1-score': macro_f1,\n",
    "        'support': sum(support)\n",
    "    }\n",
    "    metrics['micro avg'] = {\n",
    "        'precision': micro_precision,\n",
    "        'recall': micro_recall,\n",
    "        'f1-score': micro_f1,\n",
    "        'support': sum(support)\n",
    "    }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "\n",
    "def evaluate(predictions, test_set, tagset):\n",
    "    classification_reports = []  \n",
    "    for prediction in predictions:\n",
    "        classification_reports.append(evaluate_multilabel(prediction, test_set, tagset))\n",
    "    return classification_reports\n",
    "\n",
    "\n",
    "def get_avg_scores(classification_reports, score='f1-score'):\n",
    "    avg_scores = {}\n",
    "    for report in classification_reports:\n",
    "        for tag in report.keys():\n",
    "            if tag not in avg_scores:\n",
    "                avg_scores[tag] = []\n",
    "            avg_scores[tag].append(report[tag][score])\n",
    "\n",
    "    avg_scores = {tag: sum(scores)/len(scores) for tag, scores in avg_scores.items()}\n",
    "    return avg_scores\n",
    "\n",
    "\n",
    "def bar_plot(scores, tagset, score='f1-score'):\n",
    "    fig, ax = plt.subplots(figsize=(12, 5))\n",
    "    barWidth = 0.2\n",
    "    bars = [[data[tag] for tag in tagset if tag != 'O'] for data in scores.values()]\n",
    "    rnge = np.arange(0, (len(bars[0])*1.2), 1.2)\n",
    "    r = [rnge]\n",
    "    for i in range(1, len(bars)):\n",
    "        r.append([x + barWidth for x in r[i-1]])\n",
    "       \n",
    "\n",
    "    \n",
    "    colors = ['#0072B2', '#D55E00', '#CC79A7', '#E69F00', '#56B4E9'] # Colorblind-friendly palette\n",
    "    for i in range(len(bars)):\n",
    "        ax.bar(r[i], bars[i], color=colors[i], width=barWidth, label=f'{list(scores.keys())[i]}')\n",
    "    ax.set_xticks([r + barWidth for r in rnge])\n",
    "    ax.set_xticklabels(tagset, rotation=30)\n",
    "    ax.tick_params(bottom=False, left=False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.spines['bottom'].set_color('#DDDDDD')\n",
    "    ax.set_axisbelow(True)\n",
    "    ax.yaxis.grid(True, color='#EEEEEE')\n",
    "    ax.xaxis.grid(False)\n",
    "    ax.set_xlabel('Token classes', labelpad=15, color='#333333')\n",
    "    ax.set_ylabel(score, labelpad=15, color='#333333')\n",
    "    plt.legend(loc=(1.04, 0.7))\n",
    "    plt.show()\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['gpt4o', 'gpt4.1-mini']#['gpt3.5', 'gpt4', 'gpt4o', 'o1-mini', 'gpt4.1-mini']\n",
    "metrics = ['precision', 'recall', 'f1-score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain-mark: Precision=1.00, Recall=0.41, F1=0.58\n",
      "ENE-Misc: Precision=0.03, Recall=0.01, F1=0.02\n",
      "ENE-Person: Precision=0.45, Recall=0.03, F1=0.05\n",
      "ENE-Spatial: Precision=0.59, Recall=0.19, F1=0.28\n",
      "Head: Precision=0.92, Recall=0.52, F1=0.66\n",
      "Latlong: Precision=0.93, Recall=0.41, F1=0.57\n",
      "NC-Person: Precision=0.82, Recall=0.14, F1=0.24\n",
      "NC-Spatial: Precision=0.80, Recall=0.34, F1=0.48\n",
      "NP-Misc: Precision=0.19, Recall=0.11, F1=0.14\n",
      "NP-Person: Precision=0.75, Recall=0.41, F1=0.53\n",
      "NP-Spatial: Precision=0.93, Recall=0.50, F1=0.65\n",
      "O: Precision=0.79, Recall=0.98, F1=0.88\n",
      "Relation: Precision=0.62, Recall=0.36, F1=0.46\n",
      "macro avg: Precision=0.68, Recall=0.34, F1=0.43\n",
      "micro avg: Precision=0.79, Recall=0.76, F1=0.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ner-llm-py311/lib/python3.11/site-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['NC-Misc', 'NE-Misc'] will be ignored\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "path = join('predictions','token_classification_' + models[0])\n",
    "\n",
    "tagset = sorted({label for sentence in test_set['tags'] for tag in sentence for label in (tag if isinstance(tag, list) else [tag])})\n",
    "\n",
    "predictions = load_predictions(path)\n",
    "scores = evaluate_multilabel(predictions[0], test_set, tagset)\n",
    "for tag, score in scores.items():\n",
    "    print(f\"{tag}: Precision={score['precision']:.2f}, Recall={score['recall']:.2f}, F1={score['f1-score']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt4o\n",
      "precision: micro: 0.7907453196750265\tmacro: 0.6789836136611052\trecall: micro: 0.7631417467784823\tmacro: 0.3389642880678281\tf1-score: micro: 0.7766983554229409\tmacro: 0.4259636986720865\t\n",
      "gpt4.1-mini\n",
      "precision: micro: 0.7848625490896108\tmacro: 0.6223895852016401\trecall: micro: 0.7494375127838003\tmacro: 0.5321373683818218\tf1-score: micro: 0.7667410714285714\tmacro: 0.5208862026087302\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ner-llm-py311/lib/python3.11/site-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['NC-Misc', 'NE-Misc'] will be ignored\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/ner-llm-py311/lib/python3.11/site-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['NC-Misc'] will be ignored\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    print(model)\n",
    "    path = join('predictions','token_classification_' + model)\n",
    "    tagset = sorted({label for sentence in test_set['tags'] for tag in sentence for label in (tag if isinstance(tag, list) else [tag])})\n",
    "    predictions = load_predictions(path)\n",
    "    classification_reports = evaluate(predictions, test_set, tagset)\n",
    "\n",
    "    for metric in metrics:\n",
    "        scores = get_avg_scores(classification_reports, metric)\n",
    "        print(f'{metric}', end=': ')\n",
    "        print('micro:', scores['micro avg'], end='\\t')\n",
    "        print('macro:', scores['macro avg'], end='\\t')\n",
    "        #print(\"{:.2f}\".format(scores['micro avg']), end='\\t')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_scores = {}\n",
    "\n",
    "for metric in metrics:\n",
    "    for model in models:\n",
    "        path = join('predictions','token_classification_' + model)\n",
    "        tagset = sorted({label for sentence in test_set['tags'] for tag in sentence for label in (tag if isinstance(tag, list) else [tag])})\n",
    "    \n",
    "        predictions = load_predictions(path)\n",
    "        classification_reports = evaluate(predictions, test_set, tagset)\n",
    "        eval_scores[model] = get_avg_scores(classification_reports, metric)\n",
    "\n",
    "    bar_plot(eval_scores, tagset, metric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ner-llm-py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
